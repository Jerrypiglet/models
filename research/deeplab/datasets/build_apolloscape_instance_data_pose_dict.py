"""Converts Apolloscape car instance (pose and shape) data to TFRecord file format with Example protos.

The Apolloscape dataset is expected to have the following directory structure:

+ apolloscape
  + 3d_car_instance_sample
     + camera
     + car_models //.pkl file of car models
     + car_poses //json file annotation of pose and shape for cars in each image
     + images
     + split
     + pose_maps //pose map files generated by Apolloscape toolkit (http://yq01-sys-hic-k40-0003.yq01.baidu.com:8888/notebooks/baidu/personal-code/car-fitting/rui_modelfitting/dataset-api/car_instance/demo.ipynb)

This script converts data into sharded data files and save at tfrecord folder.

The Example proto contains the following fields:

  image/encoded: encoded image content.
  image/filename: image filename.
  image/format: image file format.
  image/height: image height.
  image/width: image width.
  image/channels: image channels.
  image/segmentation/class/encoded: encoded semantic segmentation content.
  image/segmentation/class/format: semantic segmentation file format.
"""
import glob
import math
import os.path
import re
import sys
import build_data
import tensorflow as tf
import numpy as np
import ntpath
import h5py

FLAGS = tf.app.flags.FLAGS
dataset_folders = ['3d_car_instance_sample', 'full', 'combined']
dataset_subfolders = ['', '/train', '/train']
dataset_folder_index = 2
dataset_folder = dataset_folders[dataset_folder_index] + dataset_subfolders[dataset_folder_index]

print '===== dataset_folder: ', dataset_folder

tf.app.flags.DEFINE_string('apolloscape_root',
                           './apolloscape/%s'%dataset_folder,
                           'Apolloscape dataset root folder.')

tf.app.flags.DEFINE_string(
    'output_dir',
    './apolloscape/%s/tfrecord_02_posedict_half'%dataset_folder,
    'Path to save converted SSTable of TensorFlow examples.')

tf.app.flags.DEFINE_string(
    'splits_dir',
    './apolloscape/%s/split'%dataset_folder,
    'Path to splits files (.txt) for train/val.')

_NUM_SHARDS = 5

# A map from data type to folder name that saves the data.
_FOLDERS_MAP = {
    'image': 'pose_maps_02',
    'seg': 'pose_maps_02',
    'vis': 'pose_maps_02',
    'pose_dict': 'pose_maps_02'
}

# A map from data type to filename postfix.
_POSTFIX_MAP = {
    'image': '_rescaled_half',
    'seg': '_seg_half',
    'vis': '_vis_half',
    # 'image': '_rescaled',
    # 'seg': '_seg',
    # 'vis': '_vis',
    'pose_dict': '_posedict'
}

# A map from data type to data format.
_DATA_FORMAT_MAP = {
    'image': 'png',
    'vis': 'png',
    'seg': 'png',
    'pose_dict': 'npy'
}

# Image file pattern.
_IMAGE_FILENAME_RE = re.compile('(.+)' + _POSTFIX_MAP['image'])

def _get_files(data, dataset_split):
  """Gets files for the specified data type and dataset split.

  Args:
    data: String, desired data ('image' or 'label').
    dataset_split: String, dataset split ('train', 'val', 'test')

  Returns:
    A list of sorted file names or None when getting label for
      test set.
  """

  text_file = open('%s/%s.txt'%(FLAGS.splits_dir, dataset_split), "r")
  filenames_split = [line.replace('.jpg', '') for line in text_file.read().split('\n') if '.jpg' in line]

  if data == 'seg' and dataset_split == 'test':
    return None
  pattern = '*%s.%s' % (_POSTFIX_MAP[data], _DATA_FORMAT_MAP[data])
  search_files = os.path.join(
      FLAGS.apolloscape_root, _FOLDERS_MAP[data], pattern)
  filepaths = glob.glob(search_files)
  filepaths_split = [filepath for filepath in filepaths if ntpath.basename(filepath).replace('%s.%s' % (_POSTFIX_MAP[data], _DATA_FORMAT_MAP[data]), '') in filenames_split]
  print dataset_split, pattern, len(filepaths_split)

  return sorted(filepaths_split)


def _convert_dataset(dataset_split):
  """Converts the specified dataset split to TFRecord format.

  Args:
    dataset_split: The dataset split (e.g., train, val).

  Raises:
    RuntimeError: If loaded image and label have different shape, or if the
      image file with specified postfix could not be found.
  """
  image_files = _get_files('image', dataset_split)
  vis_files = _get_files('vis', dataset_split)
  seg_files = _get_files('seg', dataset_split)
  pose_dict_files = _get_files('pose_dict', dataset_split)
  num_images = len(image_files)
  num_per_shard = int(math.ceil(num_images / float(_NUM_SHARDS)))
  assert len(image_files) == len(vis_files) == len(seg_files) == len(pose_dict_files), 'Three list lengths not equal!'
  print 'num_images, num_segs, num_pose_dicts, num_per_shard: ', num_images, len(seg_files), len(pose_dict_files), num_per_shard
  import random
  indices = range(num_images)
  random.shuffle(indices)
  image_files = [image_files[indice] for indice in indices]
  vis_files = [vis_files[indice] for indice in indices]
  seg_files = [seg_files[indice] for indice in indices]
  pose_dict_files = [pose_dict_files[indice] for indice in indices]

  image_reader = build_data.ImageReader('png', channels=3)
  vis_reader = build_data.ImageReader('png', channels=3)
  seg_reader = build_data.ImageReader('png', channels=1)

  def return_id(filepath):
      file_name = ntpath.basename(filepath)
      splits = file_name.split('_')
      return splits[0]+'_'+splits[1]

  for shard_id in range(_NUM_SHARDS):
    shard_filename = '%s-%05d-of-%05d.tfrecord' % (
        dataset_split, shard_id, _NUM_SHARDS)
    output_filename = os.path.join(FLAGS.output_dir, shard_filename)
    # options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)
    options = {}
    with tf.python_io.TFRecordWriter(output_filename, options=options) as tfrecord_writer:
      start_idx = shard_id * num_per_shard
      end_idx = min((shard_id + 1) * num_per_shard, num_images)
      for i in range(start_idx, end_idx):
        sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
            i + 1, num_images, shard_id))
        sys.stdout.flush()
        # Read the image.
        print return_id(image_files[i]), return_id(seg_files[i]), return_id(pose_dict_files[i])
        assert return_id(image_files[i]) == return_id(vis_files[i]) == return_id(seg_files[i]) == return_id(pose_dict_files[i]), 'File name mismatch!'

        image_data = tf.gfile.FastGFile(image_files[i], 'rb').read()
        height, width = image_reader.read_image_dims(image_data)
        vis_data = tf.gfile.FastGFile(vis_files[i], 'rb').read()
        print vis_files[i]
        seg_data = tf.gfile.FastGFile(seg_files[i], 'rb').read()
        height_seg, width_seg = seg_reader.read_image_dims(seg_data)
        assert height == height_seg and width == width_seg, 'W and H for image and seg must match!'
        # Read the semantic segmentation annotation.
        pose_dict_data = np.load(pose_dict_files[i])
        pose_dict_data = np.vstack((np.zeros((1, 6)) + 255., pose_dict_data))
        print np.mean(pose_dict_data[1:, :]), np.shape(pose_dict_data), np.shape(image_data)
        # print np.mean(seg_data.astype(np.float)), np.shape(seg_data)
        # pose_dict_data = np.hstack((np.arange(0, pose_dict_data.shape[0]).reshape((-1, 1)), pose_dict_data))
        # Convert to tf example.
        re_match = _IMAGE_FILENAME_RE.search(image_files[i])
        if re_match is None:
          raise RuntimeError('Invalid image filename: ' + image_files[i])
        filename = '%s-%s'%(dataset_split, os.path.basename(re_match.group(1)))
        example = build_data.image_posedict_to_tfexample(
            image_data, vis_data, seg_data, filename, height, width, pose_dict_data)
        tfrecord_writer.write(example.SerializeToString())
    sys.stdout.write('\n')
    sys.stdout.flush()


def main(unused_argv):
  # Only support converting 'train' and 'val' sets for now.
  for dataset_split in ['train', 'val']:
    _convert_dataset(dataset_split)


if __name__ == '__main__':
  tf.app.run()
